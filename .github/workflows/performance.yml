name: Performance Testing

on:
  schedule:
    - cron: '0 2 * * *'  # Run daily at 2 AM
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - cpp-core
          - python-backend
          - frontend
      test_duration:
        description: 'Test duration in minutes'
        required: false
        default: '10'

env:
  BUILD_TYPE: Release

jobs:
  # ====================================
  # C++ CORE PERFORMANCE
  # ====================================
  cpp-performance:
    name: C++ Core Performance
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'cpp-core' || github.event_name == 'schedule'
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup build environment
      uses: ./.github/actions/setup-build-env
    
    - name: Cache benchmark data
      uses: actions/cache@v3
      with:
        path: |
          benchmark-data/
          test-images/
        key: benchmark-data-v1
    
    - name: Download test images
      run: |
        mkdir -p test-images benchmark-data
        # Download various test images for benchmarking
        curl -o test-images/test_4k.jpg "https://sample-videos.com/zip/10/jpg/4k.jpg" || true
        curl -o test-images/test_1080p.png "https://sample-videos.com/zip/10/png/1080p.png" || true
        curl -o test-images/test_8k.tiff "https://sample-videos.com/zip/10/tiff/8k.tiff" || true
    
    - name: Build with benchmarks
      run: |
        cd core-cpp
        cmake -B build \
          -DCMAKE_BUILD_TYPE=Release \
          -DBUILD_BENCHMARKS=ON \
          -DENABLE_SIMD=ON \
          -DENABLE_PROFILING=ON
        cmake --build build --config Release
    
    - name: Run image processing benchmarks
      run: |
        cd core-cpp
        echo "Running image processing benchmarks..."
        
        # Basic operations benchmark
        ./build/benchmarks/basic_ops_bench \
          --benchmark_out=../benchmark-data/basic_ops_${{ matrix.os }}.json \
          --benchmark_out_format=json || true
        
        # SIMD optimizations benchmark
        ./build/benchmarks/simd_bench \
          --benchmark_out=../benchmark-data/simd_${{ matrix.os }}.json \
          --benchmark_out_format=json || true
        
        # Memory usage benchmark
        ./build/benchmarks/memory_bench \
          --benchmark_out=../benchmark-data/memory_${{ matrix.os }}.json \
          --benchmark_out_format=json || true
        
        # Large image processing benchmark
        ./build/benchmarks/large_image_bench \
          --benchmark_out=../benchmark-data/large_image_${{ matrix.os }}.json \
          --benchmark_out_format=json || true
    
    - name: Performance regression analysis
      run: |
        cd benchmark-data
        python3 << 'EOF'
        import json
        import os
        
        # Compare with baseline performance
        baseline_file = f"baseline_basic_ops_{os.environ['RUNNER_OS']}.json"
        current_file = f"basic_ops_{os.environ['RUNNER_OS']}.json"
        
        if os.path.exists(baseline_file) and os.path.exists(current_file):
            with open(baseline_file) as f:
                baseline = json.load(f)
            with open(current_file) as f:
                current = json.load(f)
            
            print("Performance comparison:")
            # Simple performance comparison logic
            for bench in current.get('benchmarks', []):
                name = bench['name']
                current_time = bench['real_time']
                
                # Find corresponding baseline
                baseline_bench = next((b for b in baseline.get('benchmarks', []) if b['name'] == name), None)
                if baseline_bench:
                    baseline_time = baseline_bench['real_time']
                    diff_percent = ((current_time - baseline_time) / baseline_time) * 100
                    print(f"{name}: {diff_percent:+.2f}% change")
                    
                    if diff_percent > 10:  # 10% regression threshold
                        print(f"WARNING: Performance regression detected in {name}")
        EOF
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: cpp-benchmarks-${{ matrix.os }}
        path: benchmark-data/

  # ====================================
  # PYTHON BACKEND PERFORMANCE
  # ====================================
  python-performance:
    name: Python Backend Performance
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'python-backend' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        cd backend-python
        pip install -r requirements.txt || true
        pip install locust pytest-benchmark pytest-asyncio httpx
    
    - name: Start backend server
      run: |
        cd backend-python
        uvicorn main:app --host 0.0.0.0 --port 8000 &
        sleep 10  # Wait for server to start
    
    - name: API endpoint benchmarks
      run: |
        cd backend-python
        duration=${{ github.event.inputs.test_duration || '10' }}
        
        # Load testing with Locust
        locust --headless \
          --users 50 \
          --spawn-rate 5 \
          --run-time ${duration}m \
          --html performance_report.html \
          --csv performance_data \
          -f tests/performance/locustfile.py \
          --host http://localhost:8000 || true
    
    - name: Database performance tests
      run: |
        cd backend-python
        python3 << 'EOF'
        import asyncio
        import time
        import sqlite3
        import statistics
        
        # Database operation benchmarks
        def benchmark_db_operations():
            conn = sqlite3.connect(':memory:')
            conn.execute('''CREATE TABLE test_projects 
                           (id INTEGER PRIMARY KEY, name TEXT, data BLOB)''')
            
            # Insert performance
            start_time = time.time()
            for i in range(1000):
                conn.execute("INSERT INTO test_projects (name, data) VALUES (?, ?)", 
                           (f"project_{i}", b"test_data" * 100))
            conn.commit()
            insert_time = time.time() - start_time
            
            # Query performance
            start_time = time.time()
            for i in range(100):
                conn.execute("SELECT * FROM test_projects WHERE id = ?", (i,)).fetchone()
            query_time = time.time() - start_time
            
            print(f"Insert 1000 records: {insert_time:.3f}s")
            print(f"Query 100 records: {query_time:.3f}s")
            
            conn.close()
        
        benchmark_db_operations()
        EOF
    
    - name: Memory profiling
      run: |
        cd backend-python
        pip install memory-profiler psutil
        
        # Profile memory usage of key operations
        python -c "
        import psutil
        import time
        
        process = psutil.Process()
        print(f'Initial memory: {process.memory_info().rss / 1024 / 1024:.2f} MB')
        
        # Simulate image processing workload
        data = [bytearray(1024*1024) for _ in range(100)]  # 100MB
        print(f'Peak memory: {process.memory_info().rss / 1024 / 1024:.2f} MB')
        
        del data
        time.sleep(1)
        print(f'After cleanup: {process.memory_info().rss / 1024 / 1024:.2f} MB')
        "
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: python-performance
        path: |
          backend-python/performance_report.html
          backend-python/performance_data_*.csv

  # ====================================
  # FRONTEND PERFORMANCE
  # ====================================
  frontend-performance:
    name: Frontend Performance
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'frontend' || github.event_name == 'schedule'
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend-electron/package-lock.json
    
    - name: Install dependencies
      run: |
        cd frontend-electron
        npm ci
        npx playwright install chromium
    
    - name: Build frontend
      run: |
        cd frontend-electron
        npm run build
    
    - name: Lighthouse performance audit
      run: |
        cd frontend-electron
        npm install -g @lhci/cli
        
        # Start local server
        npm run serve &
        sleep 10
        
        # Run Lighthouse CI
        lhci autorun --config=.lighthouserc.json || true
    
    - name: Canvas rendering performance
      run: |
        cd frontend-electron
        npx playwright test --project=performance || true
    
    - name: Bundle size analysis
      run: |
        cd frontend-electron
        npm run analyze || npx webpack-bundle-analyzer dist/static/js/*.js --report bundle-report.html --mode static || true
    
    - name: Upload frontend performance results
      uses: actions/upload-artifact@v3
      with:
        name: frontend-performance
        path: |
          frontend-electron/.lighthouseci/
          frontend-electron/test-results/
          frontend-electron/bundle-report.html

  # ====================================
  # PERFORMANCE COMPARISON & REPORTING
  # ====================================
  performance-report:
    name: Generate Performance Report
    needs: [cpp-performance, python-performance, frontend-performance]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all performance artifacts
      uses: actions/download-artifact@v3
    
    - name: Setup Python for reporting
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install reporting dependencies
      run: |
        pip install matplotlib pandas numpy jinja2
    
    - name: Generate comprehensive performance report
      run: |
        python3 << 'EOF'
        import json
        import pandas as pd
        import matplotlib.pyplot as plt
        from datetime import datetime
        import os
        
        # Create performance report
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'cpp_benchmarks': {},
            'python_performance': {},
            'frontend_metrics': {}
        }
        
        # Process C++ benchmarks
        for os_name in ['ubuntu-latest', 'macos-latest', 'windows-latest']:
            benchmark_file = f"cpp-benchmarks-{os_name}/basic_ops_{os_name}.json"
            if os.path.exists(benchmark_file):
                with open(benchmark_file) as f:
                    data = json.load(f)
                    report_data['cpp_benchmarks'][os_name] = data
        
        # Create summary report
        with open('performance_summary.md', 'w') as f:
            f.write("# Performance Test Results\n\n")
            f.write(f"**Test Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## C++ Core Performance\n")
            for os_name, data in report_data['cpp_benchmarks'].items():
                f.write(f"### {os_name}\n")
                if 'benchmarks' in data:
                    for bench in data['benchmarks'][:5]:  # Top 5 benchmarks
                        f.write(f"- **{bench['name']}**: {bench['real_time']:.2f} {bench['time_unit']}\n")
                f.write("\n")
            
            f.write("## Recommendations\n")
            f.write("- Monitor performance regressions > 10%\n")
            f.write("- Consider SIMD optimizations for image processing\n")
            f.write("- Profile memory usage for large images\n")
        
        print("Performance report generated successfully!")
        EOF
    
    - name: Comment performance results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = './performance_summary.md';
          
          if (fs.existsSync(path)) {
            const summary = fs.readFileSync(path, 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## 🚀 Performance Test Results\n\n${summary}`
            });
          }
    
    - name: Upload comprehensive performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: |
          performance_summary.md
          *.png
          *.json
